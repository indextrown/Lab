{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f802e45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-vision\n",
      "  Using cached google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2\n",
      "  Downloading protobuf-6.33.0-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m427.5/427.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting proto-plus<2.0.0,>=1.22.3\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1\n",
      "  Using cached google_api_core-2.26.0-py3-none-any.whl (162 kB)\n",
      "Collecting google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1\n",
      "  Using cached google_auth-2.41.1-py2.py3-none-any.whl (221 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2\n",
      "  Using cached googleapis_common_protos-1.70.0-py3-none-any.whl (294 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.32.5)\n",
      "Collecting grpcio<2.0.0,>=1.33.2\n",
      "  Downloading grpcio-1.75.1-cp310-cp310-macosx_11_0_universal2.whl (11.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio-status<2.0.0,>=1.33.2\n",
      "  Using cached grpcio_status-1.75.1-py3-none-any.whl (14 kB)\n",
      "Collecting cachetools<7.0,>=2.0.0\n",
      "  Using cached cachetools-6.2.1-py3-none-any.whl (11 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (4.15.0)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (3.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2025.10.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kimdonghyeon/.pyenv/versions/3.10.12/envs/rest-env/lib/python3.10/site-packages (from requests<3.0.0,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision) (2.5.0)\n",
      "Installing collected packages: pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, grpcio-status, google-auth, google-api-core, google-cloud-vision\n",
      "Successfully installed cachetools-6.2.1 google-api-core-2.26.0 google-auth-2.41.1 google-cloud-vision-3.10.2 googleapis-common-protos-1.70.0 grpcio-1.75.1 grpcio-status-1.75.1 proto-plus-1.26.1 protobuf-6.33.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 rsa-4.9.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install google-cloud-vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ed82a",
   "metadata": {},
   "source": [
    "## 1. ì¸ìŠ¤íƒ€ APIë¡œ í•´ì‹œíƒœê·¸ ê²°ê³¼ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97721d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ ì €ì¥ ì™„ë£Œ: /Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/popup.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "# âœ… .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "BASE_URL = os.getenv(\"API_BASE_URL\")\n",
    "\n",
    "# âœ… DTO ì •ì˜\n",
    "@dataclass\n",
    "class InstagramPostDTO:\n",
    "    id: str\n",
    "    caption: Optional[str]\n",
    "    media_type: str\n",
    "    permalink: str\n",
    "    timestamp: str\n",
    "    media_urls: List[str]\n",
    "\n",
    "\n",
    "class InstagramAPI:\n",
    "    def __init__(self, access_token: str, user_id: str, base_url: str):\n",
    "        self.access_token = access_token\n",
    "        self.user_id = user_id\n",
    "        self.base_url = base_url\n",
    "\n",
    "    def get_hashtag_id(self, hashtag: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        ì£¼ì–´ì§„ í•´ì‹œíƒœê·¸ í…ìŠ¤íŠ¸ë¡œë¶€í„° Instagram Graph APIì˜ í•´ì‹œíƒœê·¸ IDë¥¼ ì¡°íšŒ\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/ig_hashtag_search\"\n",
    "        params = {\n",
    "            \"user_id\": self.user_id,\n",
    "            \"q\": hashtag,\n",
    "            \"access_token\": self.access_token\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json().get(\"data\", [])\n",
    "\n",
    "            if not data:\n",
    "                print(f\"âš ï¸ í•´ì‹œíƒœê·¸ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{hashtag}'\")\n",
    "                return None\n",
    "\n",
    "            return data[0][\"id\"]\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"âŒ í•´ì‹œíƒœê·¸ ID ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_recent_media(self, hashtag_id: str, limit: int = 5) -> List[InstagramPostDTO]:\n",
    "        \"\"\"\n",
    "        íŠ¹ì • í•´ì‹œíƒœê·¸ IDì— ëŒ€í•œ ìµœê·¼ ê²Œì‹œë¬¼ë“¤ì„ ì¡°íšŒ â†’ DTO ë³€í™˜\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/{hashtag_id}/recent_media\"\n",
    "        params = {\n",
    "            \"user_id\": self.user_id,\n",
    "            \"access_token\": self.access_token,\n",
    "            \"fields\": (\n",
    "                \"id,caption,media_type,media_url,permalink,comments_count,\"\n",
    "                \"like_count,timestamp,children{media_url,media_type}\"\n",
    "            ),\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            json_data = response.json()\n",
    "            raw_posts = json_data.get(\"data\", [])\n",
    "\n",
    "            parsed_posts: List[InstagramPostDTO] = []\n",
    "            for item in raw_posts:\n",
    "                if item[\"media_type\"] not in (\"IMAGE\", \"CAROUSEL_ALBUM\"):\n",
    "                    continue\n",
    "\n",
    "                media_urls: List[str] = []\n",
    "\n",
    "                if item[\"media_type\"] == \"CAROUSEL_ALBUM\" and \"children\" in item:\n",
    "                    for child in item[\"children\"][\"data\"]:\n",
    "                        if \"media_url\" in child:\n",
    "                            media_urls.append(child[\"media_url\"])\n",
    "                elif \"media_url\" in item:\n",
    "                    media_urls.append(item[\"media_url\"])\n",
    "\n",
    "                post_dto = InstagramPostDTO(\n",
    "                    id=item.get(\"id\"),\n",
    "                    caption=item.get(\"caption\"),\n",
    "                    media_type=item.get(\"media_type\"),\n",
    "                    permalink=item.get(\"permalink\"),\n",
    "                    timestamp=item.get(\"timestamp\"),\n",
    "                    media_urls=media_urls\n",
    "                )\n",
    "                parsed_posts.append(post_dto)\n",
    "\n",
    "            return parsed_posts\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"âŒ ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "        except ValueError:\n",
    "            print(\"âŒ JSON íŒŒì‹± ì‹¤íŒ¨\")\n",
    "        return []\n",
    "\n",
    "    def save_json(self, data: List[InstagramPostDTO], filename_prefix=\"media\", hashtag=None):\n",
    "        \"\"\"\n",
    "        DTO ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "        \"\"\"\n",
    "        # hashtag_part = f\"_{hashtag}\" if hashtag else \"\"\n",
    "        # timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        # filename = f\"{filename_prefix}{hashtag_part}_{timestamp}.json\"\n",
    "        filename = \"popup.json\"  # âœ… íŒŒì¼ ì´ë¦„ ê³ ì •\n",
    "\n",
    "        # dataclass â†’ dict ë³€í™˜\n",
    "        json_data = [post.__dict__ for post in data]\n",
    "\n",
    "        try:\n",
    "            with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(json_data, f, indent=4, ensure_ascii=False)\n",
    "            print(f\"ğŸ“ ì €ì¥ ì™„ë£Œ: {os.path.abspath(filename)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ì €ì¥ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def play():\n",
    "        \"\"\"\n",
    "        ì‹¤í–‰ ì§„ì…ì \n",
    "        \"\"\"\n",
    "        load_dotenv()\n",
    "        access_token = os.getenv(\"INSTA_ACCESS_TOKEN\")\n",
    "        user_id = os.getenv(\"IG_USER_ID\")\n",
    "        hashtag = \"íŒì—…ìŠ¤í† ì–´\"\n",
    "        base_url = \"https://graph.facebook.com/v20.0\"\n",
    "\n",
    "        if not access_token or not user_id:\n",
    "            print(\"âŒ .env ì„¤ì • ëˆ„ë½: ACCESS_TOKEN ë˜ëŠ” IG_USER_ID\")\n",
    "            return\n",
    "\n",
    "        api = InstagramAPI(access_token, user_id, base_url)\n",
    "        hashtag_id = api.get_hashtag_id(hashtag)\n",
    "        if not hashtag_id:\n",
    "            print(f\"âŒ í•´ì‹œíƒœê·¸ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {hashtag}\")\n",
    "            return\n",
    "\n",
    "        posts = api.get_recent_media(hashtag_id)\n",
    "        api.save_json(posts, hashtag=hashtag)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    InstagramAPI.play()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc54f4",
   "metadata": {},
   "source": [
    "## 2. GPT APIë¡œ ë°ì´í„° ì •ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0bde4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš« webp íŒŒì¼ ìŠ¤í‚µ ë° URL ì œê±°: https://scontent-icn2-1.cdninstagram.com/v/t51.82787-15/567632338_18340038574206293_4434618611890754533_n.webp?stp=dst-jpg_e35_tt6&_nc_cat=104&ccb=1-7&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiQ0FST1VTRUxfSVRFTS5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=gaZQQLuJehMQ7kNvwGs6frV&_nc_oc=AdktRynIXALn0P4dY5ctePqWCixkDnx4c0FDO8X-TonnviManNFfiMA60gxRRjyS5xA&_nc_zt=23&_nc_ht=scontent-icn2-1.cdninstagram.com&edm=AEoDcc0EAAAA&_nc_gid=c_hVnWAHUTEMR97esJFHEw&oh=00_AfcdupKn6WXzi0pzzNgbjjz09nPuAuMJxha0XpBw88u_Zg&oe=68FBB42A\n",
      "ğŸš« webp íŒŒì¼ ìŠ¤í‚µ ë° URL ì œê±°: https://scontent-icn2-1.cdninstagram.com/v/t51.82787-15/568704543_18340038583206293_3042410553993053055_n.webp?stp=dst-jpg_e35_tt6&_nc_cat=102&ccb=1-7&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiQ0FST1VTRUxfSVRFTS5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=Z3dwpDlblKAQ7kNvwFh49Yt&_nc_oc=AdlKJK-sNedM_fQA4-nY5h60ZlAlPulq_YJcMEgkIevz6gY6JH3wi2tuRa4J59EVKxA&_nc_zt=23&_nc_ht=scontent-icn2-1.cdninstagram.com&edm=AEoDcc0EAAAA&_nc_gid=c_hVnWAHUTEMR97esJFHEw&oh=00_AfcPWxcGRTehGmuB8gfkL8wX8dqsDg7e4i2bhePEaCmeLA&oe=68FB8EBC\n",
      "ğŸš« webp íŒŒì¼ ìŠ¤í‚µ ë° URL ì œê±°: https://scontent-icn2-1.cdninstagram.com/v/t51.82787-15/568344652_18340038592206293_5376343847463853749_n.webp?stp=dst-jpg_e35_tt6&_nc_cat=111&ccb=1-7&_nc_sid=18de74&efg=eyJlZmdfdGFnIjoiQ0FST1VTRUxfSVRFTS5iZXN0X2ltYWdlX3VybGdlbi5DMyJ9&_nc_ohc=RhWdC6y2oIMQ7kNvwEX5fWa&_nc_oc=Adl6f8BinlaTBF3r5q-r7ZKj_lmiwvOaXobGS8U33i54QMT9PgRhurRoeSjIfBiKuUY&_nc_zt=23&_nc_ht=scontent-icn2-1.cdninstagram.com&edm=AEoDcc0EAAAA&_nc_gid=c_hVnWAHUTEMR97esJFHEw&oh=00_AfdKLB64-KydGlLEZPL7UCz5upZYUtQ0Ksn8eIm75Fhuog&oe=68FB9066\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_1.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_2.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_3.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_4.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_5.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_6.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_7.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_8.jpg\n",
      "âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_9.jpg\n",
      "ğŸ“Œ í•„ìˆ˜ í•„ë“œ í•„í„°ë§: 0ê±´ ì œì™¸ë¨\n",
      "ğŸ§¾ ì´ 2ê±´ ì¤‘ 1ê±´ì€ ì´ë¯¸ì§€ ì—†ìŒìœ¼ë¡œ ì œì™¸ë¨\n",
      "ğŸ“ ì €ì¥ ì™„ë£Œ: /Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/gpt.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“¦ DTO ì •ì˜\n",
    "# ==============================\n",
    "\n",
    "@dataclass\n",
    "class InstagramPostDTO:\n",
    "    \"\"\"ğŸ“¸ Instagram ì›ë³¸ ë°ì´í„°\"\"\"\n",
    "    id: str\n",
    "    caption: str\n",
    "    media_type: str\n",
    "    permalink: str\n",
    "    media_urls: List[str]\n",
    "\n",
    "@dataclass\n",
    "class GptParsedEventDTO:\n",
    "    \"\"\"ğŸ§  GPT íŒŒì‹± ê²°ê³¼\"\"\"\n",
    "    name: str\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    open_time: str\n",
    "    close_time: str\n",
    "    address: str\n",
    "    region: str\n",
    "    geocoding_query: str\n",
    "    caption_summary: str\n",
    "    section: Optional[int] = None\n",
    "\n",
    "@dataclass\n",
    "class PopupEventDTO:\n",
    "    \"\"\"ğŸ“Œ ìµœì¢… ë³‘í•© ê²°ê³¼\"\"\"\n",
    "    name: str\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "    open_time: str\n",
    "    close_time: str\n",
    "    address: str\n",
    "    region: str\n",
    "    geocoding_query: str\n",
    "    insta_post_id: str\n",
    "    insta_post_url: str\n",
    "    caption_summary: str\n",
    "    caption: str\n",
    "    image_url: List[str]\n",
    "    image_paths: List[str]\n",
    "    media_type: str\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ§  GPT íŒŒì´í”„ë¼ì¸\n",
    "# ==============================\n",
    "\n",
    "class GptAPI:\n",
    "    REQUIRED_FIELDS = [\"name\", \"start_date\", \"end_date\", \"address\", \"region\", \"caption_summary\"]  # âœ… ì¶”ê°€\n",
    "    def __init__(self, access_token, model=\"gpt-4o-mini\"):\n",
    "        self.access_token = access_token\n",
    "        self.model = model\n",
    "        self.endpoint = \"https://api.openai.com/v1/chat/completions\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            \"Authorization\": f\"Bearer {self.access_token}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        })\n",
    "\n",
    "    # ---------- ë¬¸ìì—´ ì •ì œ ----------\n",
    "    @staticmethod\n",
    "    def slugify(text: str) -> str:\n",
    "        if not text:\n",
    "            return \"no_name\"\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', '_', text)\n",
    "        text = re.sub(r'[^\\w\\-ê°€-í£]', '', text)\n",
    "        return text\n",
    "\n",
    "    # ---------- íŒŒì¼ ì…ì¶œë ¥ ----------\n",
    "    def file_open(self, filename: str) -> List[InstagramPostDTO]:\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "\n",
    "        posts: List[InstagramPostDTO] = []\n",
    "        for item in raw_data:\n",
    "            posts.append(\n",
    "                InstagramPostDTO(\n",
    "                    id=item.get(\"id\", \"\"),\n",
    "                    caption=item.get(\"caption\", \"\"),\n",
    "                    media_type=item.get(\"media_type\", \"\"),\n",
    "                    permalink=item.get(\"permalink\", \"\"),\n",
    "                    media_urls=item.get(\"media_urls\", []),\n",
    "                )\n",
    "            )\n",
    "        return posts\n",
    "\n",
    "    def file_save(self, data: List[PopupEventDTO]):\n",
    "        path = \"gpt.json\"\n",
    "        json_data = [obj.__dict__ for obj in data]\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"ğŸ“ ì €ì¥ ì™„ë£Œ: {os.path.abspath(path)}\")\n",
    "        return os.path.abspath(path)\n",
    "\n",
    "    # ---------- GPT í”„ë¡¬í”„íŠ¸ (ì›ë¬¸ ìœ ì§€) ----------\n",
    "    def build_prompt(self, sections):\n",
    "        lines = []\n",
    "        for idx, cap in sections:\n",
    "            lines.append(f\"[section {idx}]\\n{cap}\")\n",
    "        body = \"\\n\\n---\\n\\n\".join(lines)\n",
    "\n",
    "        # âœ… ì—¬ê¸°ì„œ required_fields ë¬¸ìì—´í™”\n",
    "        required_list_str = \", \".join(self.REQUIRED_FIELDS)\n",
    "        return f\"\"\"\n",
    "        ì•„ë˜ì—ëŠ” ì—¬ëŸ¬ ê°œì˜ 'ì„¹ì…˜' í…ìŠ¤íŠ¸ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.\n",
    "        ê° ì„¹ì…˜ì—ëŠ” í•˜ë‚˜ ì´ìƒì˜ íŒì—… ì´ë²¤íŠ¸ ì •ë³´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "        ê° íŒì—… ì´ë²¤íŠ¸ì— ëŒ€í•´ ë‹¤ìŒ **ì†Œë¬¸ì í‚¤**ë§Œ í¬í•¨ëœ ê°ì²´ë¥¼ ìƒì„±í•˜ê³ ,\n",
    "        ëª¨ë“  ê°ì²´ë¥¼ **JSON ë°°ì—´**ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        ğŸ“Œ í•„ë“œë³„ ì‘ì„± ê·œì¹™\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "        - name: íŒì—… ì´ë¦„ ë˜ëŠ” í–‰ì‚¬ëª…\n",
    "\n",
    "        - start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)\n",
    "\n",
    "        - end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)\n",
    "\n",
    "        - open_time: ìš´ì˜ ì‹œì‘ ì‹œê°„ (HH:MM)\n",
    "\n",
    "        - close_time: ìš´ì˜ ì¢…ë£Œ ì‹œê°„ (HH:MM)\n",
    "\n",
    "        - address: ë„ë¡œëª… ì£¼ì†Œ ë˜ëŠ” ê±´ë¬¼ëª…\n",
    "            - âš ï¸ addressê°€ ì¶”ì¶œë˜ì§€ ì•ŠëŠ” ê²½ìš°, ì´ ì´ë²¤íŠ¸ëŠ” JSONì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        - region: ì§€ì—­ëª… (ì˜ˆ: ì„œìš¸, ë¶€ì‚°, ë„ì¿„)\n",
    "            - âš ï¸ regionì´ ëˆ„ë½ë˜ë©´ ì´ ì´ë²¤íŠ¸ëŠ” JSONì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        - geocoding_query: addressì™€ region, nameì„ ê¸°ë°˜ìœ¼ë¡œ ì§€ì˜¤ì½”ë”© API ê²€ìƒ‰ì— ìµœì í™”ëœ ë¬¸ìì—´\n",
    "            1) ì§€ì—­(region)ì„ ë°˜ë“œì‹œ ê°€ì¥ ì•ì— ë¶™ì´ì„¸ìš”. (ì˜ˆ: 'ë¶€ì‚°', 'ì„œìš¸', 'ì„±ë‚¨')\n",
    "            2) address ë˜ëŠ” nameì—ì„œ ë¸Œëœë“œëª…, ê±´ë¬¼ëª…, ê³µê°„ëª… ë“±ì˜ í•µì‹¬ ì§€ëª… ìš”ì†Œë§Œ ë¶™ì´ì„¸ìš”.\n",
    "                - ì˜ˆ: \"ì„±ë‚¨ í˜„ëŒ€ë°±í™”ì  íŒêµ\" âœ…\n",
    "                - ì˜ˆ: \"ì„±ë‚¨ í˜„ëŒ€ë°±í™”ì  íŒêµ ë„ì”¨\" âŒ (ë¸Œëœë“œëª… ì œê±°)\n",
    "            3) ì¸µìˆ˜, ë°©í–¥, ì¡°ì‚¬ ë“± ë¶ˆí•„ìš”í•œ ë‹¨ì–´ëŠ” ì œê±°í•˜ì„¸ìš”:\n",
    "                - 'B1', '1ì¸µ', '2F', 'B2F', 'ì§€í•˜ 1ì¸µ'\n",
    "                - 'ì•', 'ê·¼ì²˜', 'ë§ì€í¸', 'ì˜†', 'ë’·í¸', 'ì•ìª½', 'ë’¤í¸'\n",
    "                - '~ì—ì„œ', '~ì•', '~ê·¼ì²˜', '~ë§ì€í¸'\n",
    "            4) ë¸Œëœë“œëª…, íŒì—… ì´ë¦„ ë“± ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë–¨ì–´ëœ¨ë¦¬ëŠ” ë¶ˆí•„ìš”í•œ ë‹¨ì–´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "            5) ë¬¸ì¥í˜•ì´ ì•„ë‹ˆë¼ ì§§ê³  ê²€ìƒ‰ ìµœì í™”ëœ ëª…ì‚¬êµ¬ë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "            6) ë‹¨ìˆœíˆ ì§€ì—­ëª…ë§Œ ì“°ëŠ” ê²ƒì€ âŒ ê¸ˆì§€ì…ë‹ˆë‹¤. (ì˜ˆ: \"ì„±ë‚¨\" âŒ)\n",
    "            â†’ ìµœì†Œí•œ ì§€ì—­ëª… + ì§€ëª…/ê±´ë¬¼ëª…ê¹Œì§€ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: \"ì„±ë‚¨ í˜„ëŒ€ë°±í™”ì  íŒêµ\" âœ…)\n",
    "            7) addressê°€ ë¹„ì–´ ìˆëŠ” ê²½ìš°ì—ëŠ” nameì—ì„œ ì§€ëª… ìš”ì†Œë§Œ ì¶”ì¶œí•´ ì‚¬ìš©í•˜ì„¸ìš”. ë¸Œëœë“œëª…ì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        - section: ì´ ì´ë²¤íŠ¸ê°€ ì¶”ì¶œëœ ì„¹ì…˜ ë²ˆí˜¸(ì •ìˆ˜)\n",
    "\n",
    "        - caption_summary: \"caption_summaryëŠ” ë‹¨ìˆœ ìš”ì•½ì´ ì•„ë‹ˆë¼, ì¸ìŠ¤íƒ€ ì›ë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì™„ì„±ëœ ê²Œì‹œê¸€í˜• ë¬¸ë‹¨ì…ë‹ˆë‹¤. \\\n",
    "            âœ¨ êµ¬ì„± ê·œì¹™:\\n\n",
    "            - ì „ì²´ëŠ” ì•½ 6~10ì¤„ë¡œ êµ¬ì„±í•˜ì„¸ìš”.\\n\n",
    "            - ìƒë‹¨ì—ëŠ” íŒì—… ì´ë¦„, ìœ„ì¹˜, ì¼ì •, ìš´ì˜ì‹œê°„ì„ ê°„ê²°íˆ í‘œì‹œí•˜ì„¸ìš”.\\n\n",
    "            - í•˜ë‹¨ì—ëŠ” íŒì—…ì˜ ë¶„ìœ„ê¸°, ì „ì‹œÂ·ì²´í—˜ ë‚´ìš©, ìš´ì˜ íŠ¹ì§• ë“±ì„ ìì—°ìŠ¤ëŸ½ê²Œ 3ì¤„ ì´ìƒìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\\n\n",
    "            - ë¬¸ì¥ì€ ì§§ê³  ìì—°ìŠ¤ëŸ½ê²Œ, ë§í•˜ë“¯ì´ í‘œí˜„í•˜ì„¸ìš”.\\n\n",
    "            - ê°ì„±ì ì¸ í‘œí˜„ì€ í—ˆìš©í•˜ì§€ë§Œ, ê³¼ì¥ë˜ê±°ë‚˜ í™ë³´ì„± ì–´íˆ¬ëŠ” í”¼í•˜ì„¸ìš”.\\n\n",
    "            - ë¬¸ì¥ ì‚¬ì´ì—ëŠ” ì¤„ë°”ê¿ˆ(\\n)ì„ í¬í•¨í•˜ì„¸ìš”.\\n\n",
    "            ì˜ˆì‹œ:\\n\n",
    "            ğŸ¥ Jam in Bread íŒì—…ìŠ¤í† ì–´ ì˜¤í”ˆ\\n\n",
    "            ğŸ“ ì‹ ì„¸ê³„ë°±í™”ì  ê°•ë‚¨ì  B1\\n\n",
    "            ğŸ“… 10.7(ì›”) ~ 10.13(ì¼)\\n\n",
    "            ğŸ•¥ 10:30AM ~ 8:00PM\\n\n",
    "            \\n\n",
    "            ë”°ëœ»í•œ í–¥ì´ í¼ì§€ëŠ” ê³µê°„ì—ì„œ ì¼ê³¼ ë¹µì„ í•¨ê»˜ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\n",
    "            ë‹¤ì–‘í•œ ìˆ˜ì œì¼ê³¼ ë² ì´ì»¤ë¦¬ êµ¿ì¦ˆê°€ ì „ì‹œë˜ì–´ ìˆê³ , ì¼ë¶€ ìƒí’ˆì€ í˜„ì¥ êµ¬ë§¤ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\n",
    "            í•˜ë£¨ì˜ ì‹œì‘ì„ ë¶€ë“œëŸ½ê²Œ ì±„ì›Œì£¼ëŠ” ì‘ì€ íœ´ì‹ ê°™ì€ íŒì—…ì…ë‹ˆë‹¤.\\n\"\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        â— í¬í•¨ ê¸°ì¤€\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        - ë°˜ë“œì‹œ ë‹¤ìŒ í•„ë“œë“¤ì´ ëª¨ë‘ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤:\n",
    "        {required_list_str}\n",
    "        - ìœ„ í•„ë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ëˆ„ë½ëœ ì´ë²¤íŠ¸ëŠ” JSONì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        â†’ ë¶ˆëª…í™•í•˜ê±°ë‚˜ ì£¼ì†Œ/ë‚ ì§œê°€ ì—†ëŠ” ì´ë²¤íŠ¸ëŠ” ì œì™¸í•˜ì„¸ìš”.\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        ğŸ“… ë‚ ì§œ ê·œì¹™\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        - '10/7~10/23'ì²˜ëŸ¼ ì›”/ì¼ë§Œ ìˆìœ¼ë©´ 2025ë…„ìœ¼ë¡œ ë³´ì™„í•˜ì„¸ìš”.\n",
    "        - ê³¼ê±°ë…„ë„(2023, 2024 ë“±)ê°€ ëª…ì‹œë˜ì–´ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.\n",
    "        - ëª¨í˜¸í•œ ë‚ ì§œëŠ” ì œì™¸í•˜ì„¸ìš”.\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        ğŸ•’ ì‹œê°„ ê·œì¹™\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        - '10:00~20:00' í˜•íƒœëŠ” open_time / close_timeìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”.\n",
    "        - ëª…ì‹œ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ë¡œ ë‘¡ë‹ˆë‹¤.\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        ğŸ–¼ ì´ë¯¸ì§€ ê·œì¹™\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        - ì´ë¯¸ì§€ê°€ ì—¬ëŸ¬ ì¥ì´ë©´ ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "        - ë‹¨ì¼ ì´ë¯¸ì§€ë„ ë°°ì—´ë¡œ ê°ì‹¸ì„œ ë°˜í™˜í•˜ì„¸ìš”.\n",
    "\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        âš ï¸ ì¶œë ¥ í˜•ì‹\n",
    "        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        - ë°˜ë“œì‹œ JSON ë°°ì—´([])ë§Œ. ì„¤ëª…/ì£¼ì„/ì½”ë“œë¸”ë¡ ê¸ˆì§€.\n",
    "\n",
    "        {body}\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    # ---------- GPT í˜¸ì¶œ ----------\n",
    "    def call_gpt(self, prompt, max_tokens=1500, retries=2):\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"temperature\": 0,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"ë„ˆëŠ” í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì•¼.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            \"max_tokens\": max_tokens\n",
    "        }\n",
    "        for attempt in range(retries + 1):\n",
    "            try:\n",
    "                resp = self.session.post(self.endpoint, json=payload, timeout=60)\n",
    "                if resp.status_code == 200:\n",
    "                    return resp.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    print(f\"âš ï¸ ì‘ë‹µ ì˜¤ë¥˜: {resp.status_code}\")\n",
    "                time.sleep(1)\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"âš ï¸ ìš”ì²­ ì‹¤íŒ¨: {e}\")\n",
    "                time.sleep(1)\n",
    "        raise RuntimeError(\"âŒ GPT ì‘ë‹µ ì‹¤íŒ¨\")\n",
    "\n",
    "    # ---------- GPT ê²°ê³¼ íŒŒì‹± ----------\n",
    "    def extract_json_array(self, text) -> List[GptParsedEventDTO]:\n",
    "        code_match = re.search(r\"```(?:json)?\\s*(\\[[\\s\\S]*?\\])\\s*```\", text)\n",
    "        candidate = code_match.group(1) if code_match else self._greedy_bracket_slice(text)\n",
    "        try:\n",
    "            data = json.loads(candidate)\n",
    "            return self._normalize_schema(data)\n",
    "        except json.JSONDecodeError:\n",
    "            return []\n",
    "\n",
    "    def _normalize_schema(self, data) -> List[GptParsedEventDTO]:\n",
    "        if not isinstance(data, list):\n",
    "            return []\n",
    "        parsed = []\n",
    "        for obj in data:\n",
    "            parsed.append(\n",
    "                GptParsedEventDTO(\n",
    "                    name=(obj.get(\"name\") or \"\").strip(),\n",
    "                    start_date=(obj.get(\"start_date\") or \"\").strip(),\n",
    "                    end_date=(obj.get(\"end_date\") or \"\").strip(),\n",
    "                    open_time=(obj.get(\"open_time\") or \"\").strip(),\n",
    "                    close_time=(obj.get(\"close_time\") or \"\").strip(),\n",
    "                    address=(obj.get(\"address\") or \"\").strip(),\n",
    "                    region=(obj.get(\"region\") or \"\").strip(),\n",
    "                    geocoding_query=(obj.get(\"geocoding_query\") or \"\").strip(),\n",
    "                    caption_summary=(obj.get(\"caption_summary\") or \"\").strip(),\n",
    "                    section=int(obj[\"section\"]) if obj.get(\"section\") is not None else None\n",
    "                )\n",
    "            )\n",
    "        return parsed\n",
    "\n",
    "    def _greedy_bracket_slice(self, text):\n",
    "        start, end = text.find(\"[\"), text.rfind(\"]\")\n",
    "        return text[start:end + 1] if start != -1 and end != -1 else \"[]\"\n",
    "    \n",
    "    # ---------- í•„ìˆ˜ í•„ë“œ í•„í„° ----------\n",
    "    def filter_required_fields(self, events: List[PopupEventDTO]) -> List[PopupEventDTO]:\n",
    "        valid = []\n",
    "        for e in events:\n",
    "            missing = [f for f in self.REQUIRED_FIELDS if not getattr(e, f, \"\").strip()]\n",
    "            if missing:\n",
    "                print(f\"âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½({missing}) â†’ {e.name or 'ì´ë¦„ ì—†ìŒ'} ì œì™¸\")\n",
    "                continue\n",
    "            valid.append(e)\n",
    "        return valid\n",
    "\n",
    "    # ---------- ì›ë³¸ ë°ì´í„° ë³‘í•© ----------\n",
    "    def _enrich_with_original(self, posts: List[InstagramPostDTO], extracted: List[GptParsedEventDTO], section_to_post):\n",
    "        results: List[PopupEventDTO] = []\n",
    "        for event in extracted:\n",
    "            orig = section_to_post.get(event.section, InstagramPostDTO(\"\", \"\", \"\", \"\", []))\n",
    "            results.append(\n",
    "                PopupEventDTO(\n",
    "                    name=event.name,\n",
    "                    start_date=event.start_date,\n",
    "                    end_date=event.end_date,\n",
    "                    open_time=event.open_time,\n",
    "                    close_time=event.close_time,\n",
    "                    address=event.address,\n",
    "                    region=event.region,\n",
    "                    geocoding_query=event.geocoding_query,\n",
    "                    insta_post_id=orig.id,\n",
    "                    insta_post_url=orig.permalink,\n",
    "                    caption_summary=event.caption_summary,\n",
    "                    caption=orig.caption,\n",
    "                    image_url=orig.media_urls,\n",
    "                    image_paths=[],  # ë‹¤ìš´ë¡œë“œ í›„ ì±„ì›Œì§\n",
    "                    media_type=orig.media_type,\n",
    "                )\n",
    "            )\n",
    "        return results\n",
    "\n",
    "    # ---------- ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ----------\n",
    "    def download_images(self, event: PopupEventDTO, base_dir=\"images\"):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        name_slug = self.slugify(event.name) or \"no_name\"\n",
    "        folder_name = f\"{timestamp}_{event.insta_post_id}\"\n",
    "        folder_path = os.path.join(base_dir, folder_name)\n",
    "\n",
    "        try:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ í´ë” ìƒì„± ì‹¤íŒ¨ ({folder_path}): {e}\")\n",
    "            return\n",
    "\n",
    "        image_paths = []\n",
    "        valid_image_urls = []  # âœ… webp ì œê±° í›„ ë‹¤ì‹œ ë‹´ì„ ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "        for idx, url in enumerate(event.image_url, start=1):\n",
    "            try:\n",
    "                if not url or not url.startswith(\"http\"):\n",
    "                    print(f\"âš ï¸ ì˜ëª»ëœ URL â†’ ìŠ¤í‚µ: {url}\")\n",
    "                    continue\n",
    "\n",
    "                parsed = urlparse(url)\n",
    "                ext = os.path.splitext(parsed.path)[1].lower().split(\"?\")[0]\n",
    "\n",
    "                # ğŸš« webp ì œì™¸ â€” ì•„ì˜ˆ URL ë¦¬ìŠ¤íŠ¸ì—ì„œë„ ì œê±°\n",
    "                if ext == \".webp\":\n",
    "                    print(f\"ğŸš« webp íŒŒì¼ ìŠ¤í‚µ ë° URL ì œê±°: {url}\")\n",
    "                    continue\n",
    "\n",
    "                if ext in (\"\", \".heic\"):\n",
    "                    ext = \".jpg\"\n",
    "\n",
    "                filename = f\"{name_slug}_{idx}{ext}\"\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "\n",
    "                resp = requests.get(url, timeout=20)\n",
    "                if resp.status_code != 200:\n",
    "                    print(f\"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (status={resp.status_code}): {url}\")\n",
    "                    continue\n",
    "\n",
    "                content_type = resp.headers.get(\"Content-Type\", \"\").lower()\n",
    "                if not content_type.startswith(\"image/\"):\n",
    "                    print(f\"âš ï¸ ì´ë¯¸ì§€ ì•„ë‹˜ (Content-Type={content_type}): {url}\")\n",
    "                    continue\n",
    "\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "\n",
    "                image_paths.append(os.path.abspath(filepath))\n",
    "                valid_image_urls.append(url)  # âœ… ì‹¤ì œë¡œ ì„±ê³µí•œ URLë§Œ ìœ ì§€\n",
    "                print(f\"âœ… ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ: {filepath}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({url}): {e}\")\n",
    "\n",
    "        event.image_paths = image_paths\n",
    "        event.image_url = valid_image_urls  # âœ… webp ì œê±° ë°˜ì˜\n",
    "\n",
    "\n",
    "\n",
    "    # ---------- ì „ì²´ íŒŒì´í”„ë¼ì¸ ----------\n",
    "    def process_file(self, filename, batch_size=10, download=False):\n",
    "        posts = self.file_open(filename)\n",
    "        sections = []\n",
    "        section_to_post = {}\n",
    "        for idx, post in enumerate(posts):\n",
    "            if post.caption:\n",
    "                sections.append((idx, post.caption))\n",
    "                section_to_post[idx] = post\n",
    "\n",
    "        if not sections:\n",
    "            print(\"âš ï¸ ìº¡ì…˜ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return []\n",
    "\n",
    "        all_extracted: List[GptParsedEventDTO] = []\n",
    "        for i in range(0, len(sections), batch_size):\n",
    "            chunk = sections[i:i + batch_size]\n",
    "            prompt = self.build_prompt(chunk)\n",
    "            try:\n",
    "                resp_text = self.call_gpt(prompt)\n",
    "                extracted = self.extract_json_array(resp_text)\n",
    "                all_extracted.extend(extracted)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ GPT ì²˜ë¦¬ ì‹¤íŒ¨: {e}\")\n",
    "\n",
    "        results = self._enrich_with_original(posts, all_extracted, section_to_post)\n",
    "\n",
    "        if download:\n",
    "            for event in results:\n",
    "                self.download_images(event)\n",
    "\n",
    "        # âœ… í•„ìˆ˜ í•„ë“œ í•„í„°ë§ ì¶”ê°€\n",
    "        before_len = len(results)\n",
    "        results = self.filter_required_fields(results)\n",
    "        after_len = len(results)\n",
    "        print(f\"ğŸ“Œ í•„ìˆ˜ í•„ë“œ í•„í„°ë§: {before_len - after_len}ê±´ ì œì™¸ë¨\")\n",
    "\n",
    "        # ğŸ§¹ ì´ë¯¸ì§€ ì—†ëŠ” íŒì—… ì œê±°\n",
    "        before_len = len(results)\n",
    "        results = [event for event in results if len(event.image_url) > 0 or len(event.image_paths) > 0]\n",
    "        after_len = len(results)\n",
    "\n",
    "        print(f\"ğŸ§¾ ì´ {before_len}ê±´ ì¤‘ {before_len - after_len}ê±´ì€ ì´ë¯¸ì§€ ì—†ìŒìœ¼ë¡œ ì œì™¸ë¨\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "    # ---------- ì‹¤í–‰ ----------\n",
    "    @staticmethod\n",
    "    def play(download=False):\n",
    "        load_dotenv()\n",
    "        token = os.getenv(\"GPT_ACCESS_TOKEN\")\n",
    "        if not token:\n",
    "            print(\"âŒ í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: GPT_ACCESS_TOKEN\")\n",
    "            return\n",
    "        api = GptAPI(token)\n",
    "        results = api.process_file(\"popup.json\", batch_size=10, download=download)\n",
    "        api.file_save(results)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ ì‹¤í–‰\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    GptAPI.play(download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c6c0a",
   "metadata": {},
   "source": [
    "## 3. ë„ë¡œëª…ì£¼ì†Œ ë° ìœ„ê²½ë„ ì¶”ê°€(ë‘˜ì¤‘ í•˜ë‚˜ë¼ë„ ì—†ìœ¼ë©´ í•„í„°ë§)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2591a953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ì¹˜í™˜ ëŒ€ìƒ ì•„ë‹˜: ê²½ê¸°ë„ ì„±ë‚¨ì‹œ ë¶„ë‹¹êµ¬ íŒêµì—­ë¡œ146ë²ˆê¸¸ 20 í˜„ëŒ€ë°±í™”ì  íŒêµì \n",
      "âœ… Geocoding ì™„ë£Œ: geo.json (ì´ 1ê±´, ìŠ¤í‚µ 0ê±´)\n"
     ]
    }
   ],
   "source": [
    "# GeoCoding.py\n",
    "import requests\n",
    "import urllib.parse\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "from dotenv import load_dotenv\n",
    "from typing import Optional, List\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“¦ DTO ì •ì˜\n",
    "# ==============================\n",
    "\n",
    "@dataclass\n",
    "class PlaceInfoDTO:\n",
    "    road_address: Optional[str]\n",
    "    longitude: Optional[float]\n",
    "    latitude: Optional[float]\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ§­ GeoCoding ë¡œì§\n",
    "# ==============================\n",
    "\n",
    "class GeoCoding:\n",
    "    \"\"\"\n",
    "    GPT ê²°ê³¼ JSONì— ë„ë¡œëª…ì£¼ì†Œ ë° ì¢Œí‘œ(ê²½ë„/ìœ„ë„)ë¥¼ ì¶”ê°€í•˜ëŠ” í´ë˜ìŠ¤.\n",
    "    - Naver Local Search APIë¥¼ ì‚¬ìš© (ì¥ì†Œëª… ê¸°ë°˜)\n",
    "    - ê²°ê³¼ë¥¼ popup_with_geo.jsonìœ¼ë¡œ ì €ì¥\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.client_id = os.getenv(\"CLIENT_ID\")\n",
    "        self.client_secret = os.getenv(\"CLIENT_SECRET\")\n",
    "        if not self.client_id or not self.client_secret:\n",
    "            raise ValueError(\"âŒ CLIENT_ID / CLIENT_SECRET í™˜ê²½ ë³€ìˆ˜ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # ğŸ“ 1. ì¥ì†Œ ê²€ìƒ‰ (Naver Local API)\n",
    "    # -----------------------------------\n",
    "    def get_place_info(self, query: str) -> PlaceInfoDTO:\n",
    "        \"\"\"ì¥ì†Œëª…ìœ¼ë¡œ ê²€ìƒ‰ í›„ ì£¼ì†Œì™€ ì¢Œí‘œ ë°˜í™˜\"\"\"\n",
    "        encoded_query = urllib.parse.quote(query)\n",
    "        url = f\"https://openapi.naver.com/v1/search/local.json?query={encoded_query}&display=1\"\n",
    "        headers = {\n",
    "            \"X-Naver-Client-Id\": self.client_id,\n",
    "            \"X-Naver-Client-Secret\": self.client_secret\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=10)\n",
    "            data = response.json()\n",
    "\n",
    "            if 'items' in data and data['items']:\n",
    "                item = data['items'][0]\n",
    "                road_address = item.get('roadAddress') or item.get('address')\n",
    "\n",
    "                # ì¢Œí‘œê°’ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°ë„ ëŒ€ë¹„\n",
    "                try:\n",
    "                    longitude = float(item['mapx']) / 10_000_000 if item.get('mapx') else None\n",
    "                    latitude = float(item['mapy']) / 10_000_000 if item.get('mapy') else None\n",
    "                except Exception:\n",
    "                    longitude, latitude = None, None\n",
    "\n",
    "                road_address = self.normalize_address(road_address)\n",
    "\n",
    "                return PlaceInfoDTO(\n",
    "                    road_address=road_address,\n",
    "                    longitude=longitude,\n",
    "                    latitude=latitude\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Geocoding ì‹¤íŒ¨ ({query}): {e}\")\n",
    "\n",
    "        return PlaceInfoDTO(\n",
    "            road_address=None,\n",
    "            longitude=None,\n",
    "            latitude=None\n",
    "        )\n",
    "\n",
    "    # -----------------------------------\n",
    "    # ğŸª„ ì£¼ì†Œ ë³€í™˜ ë¡œì§\n",
    "    # -----------------------------------\n",
    "    def normalize_address(self, address: Optional[str]) -> Optional[str]:\n",
    "        if not address:\n",
    "            return address\n",
    "\n",
    "        replacements = {\n",
    "            \"ì„œìš¸íŠ¹ë³„ì‹œ\": \"ì„œìš¸\",\n",
    "            \"ë¶€ì‚°ê´‘ì—­ì‹œ\": \"ë¶€ì‚°\",\n",
    "            \"ëŒ€ì „ê´‘ì—­ì‹œ\": \"ëŒ€ì „\",\n",
    "            \"ëŒ€êµ¬ê´‘ì—­ì‹œ\": \"ëŒ€êµ¬\",\n",
    "            \"ì¸ì²œê´‘ì—­ì‹œ\": \"ì¸ì²œ\",\n",
    "            \"ê´‘ì£¼ê´‘ì—­ì‹œ\": \"ê´‘ì£¼\",\n",
    "            \"ìš¸ì‚°ê´‘ì—­ì‹œ\": \"ìš¸ì‚°\",\n",
    "            \"ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ\": \"ì„¸ì¢…\",\n",
    "            \"ì œì£¼íŠ¹ë³„ìì¹˜ë„\": \"ì œì£¼\"\n",
    "        }\n",
    "\n",
    "        original = address\n",
    "        for old, new in replacements.items():\n",
    "            if old in address:\n",
    "                address = address.replace(old, new)\n",
    "\n",
    "        parts = address.split(\" \", 1)\n",
    "        if parts and parts[0].endswith(\"ì‹œ\"):\n",
    "            parts[0] = parts[0].removesuffix(\"ì‹œ\")\n",
    "        address = \" \".join(parts)\n",
    "\n",
    "        if address == original:\n",
    "            print(f\"âš ï¸ ì¹˜í™˜ ëŒ€ìƒ ì•„ë‹˜: {original}\")\n",
    "\n",
    "        return address\n",
    "\n",
    "    # -----------------------------------\n",
    "    # ğŸ—º 2. popup_refined.json â†’ ì¢Œí‘œì¶”ê°€\n",
    "    # -----------------------------------\n",
    "    def add_geocoding_to_json(\n",
    "        self,\n",
    "        input_file: str = \"gpt.json\",\n",
    "        output_file: str = \"geo.json\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        ì…ë ¥ JSONì— road_address / longitude / latitude ì¶”ê°€ í›„ ì €ì¥\n",
    "        geocoding_query í•„ë“œë¥¼ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ address ì‚¬ìš©\n",
    "        ìœ„ê²½ë„ ê°’ì´ ì—†ì„ ê²½ìš° í•„í„°ë§\n",
    "        \"\"\"\n",
    "        if not os.path.exists(input_file):\n",
    "            print(f\"âŒ ì…ë ¥ íŒŒì¼ ì—†ìŒ: {input_file}\")\n",
    "            return\n",
    "\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        enriched = []\n",
    "        skipped = 0\n",
    "        for event in data:\n",
    "            query = event.get(\"geocoding_query\") or event.get(\"address\")\n",
    "            if not query:\n",
    "                print(f\"âš ï¸ ì§€ì˜¤ì½”ë”© ëŒ€ìƒ ì—†ìŒ: {event.get('name')}\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            place_info = self.get_place_info(query)\n",
    "\n",
    "            # ğŸ“Œ ìœ„ê²½ë„ ê°’ ì—†ëŠ” ê²½ìš° ì œì™¸\n",
    "            if place_info.longitude is None or place_info.latitude is None:\n",
    "                print(f\"ğŸš« ìœ„ê²½ë„ ì—†ìŒ â†’ ìŠ¤í‚µ: {event.get('name')} ({query})\")\n",
    "                skipped += 1\n",
    "                continue\n",
    "\n",
    "            # âœ… ì •ìƒ ë°ì´í„°ë§Œ ì¶”ê°€\n",
    "            new_event = {**event}\n",
    "            new_event[\"road_address\"] = place_info.road_address\n",
    "            new_event[\"longitude\"] = place_info.longitude\n",
    "            new_event[\"latitude\"] = place_info.latitude\n",
    "\n",
    "            enriched.append(new_event)\n",
    "\n",
    "        # 3ï¸âƒ£ ì €ì¥\n",
    "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(enriched, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "        print(f\"âœ… Geocoding ì™„ë£Œ: {output_file} (ì´ {len(enriched)}ê±´, ìŠ¤í‚µ {skipped}ê±´)\")\n",
    "\n",
    "    # -----------------------------------\n",
    "    # ğŸš€ 3. ì‹¤í–‰ ë©”ì„œë“œ\n",
    "    # -----------------------------------\n",
    "    @staticmethod\n",
    "    def play():\n",
    "        geo = GeoCoding()\n",
    "        geo.add_geocoding_to_json()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    GeoCoding.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec73b6",
   "metadata": {},
   "source": [
    "## mysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2fc4aa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ BASE_DIR: /Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI\n",
      "ğŸ“„ JSON ê²½ë¡œ: /Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/geo.json â†’ ì¡´ì¬? True\n",
      "ğŸŒ API URL: https://poppang.co.kr/api/v1/popup\n",
      "\n",
      "ğŸ“Œ ì²˜ë¦¬ì¤‘: insta_post_id=18105581458626493\n",
      "ğŸ“¸ ì´ë¯¸ì§€ ê²½ë¡œ: ['/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_1.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_2.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_3.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_4.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_5.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_6.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_7.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_8.jpg', '/Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_9.jpg']\n",
      "ğŸš« ì‚¬ëŒ ê°ì§€ë¨: /Users/kimdonghyeon/2025/á„€á…¢á„‡á…¡á†¯/SwiftLab/Lab/RestAPI/images/20251020-161558_18105581458626493/ë„ì”¨_íŒì—…ìŠ¤í† ì–´_1.jpg\n",
      "ğŸš« Vision ê°ì§€ë¨ â†’ ì—…ë¡œë“œ ìŠ¤í‚µ insta_post_id=18105581458626493\n",
      "\n",
      "===============================\n",
      "âœ… ì—…ë¡œë“œ ì™„ë£Œ: 0/1\n",
      "ğŸš« Vision í•„í„°ë¡œ ìŠ¤í‚µ: 1\n",
      "âš ï¸ ì˜¤ë¥˜ ë˜ëŠ” ê¸°íƒ€ ìŠ¤í‚µ: 0\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "# âœ… Vision ê¸°ëŠ¥ import\n",
    "import VisionAPI\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ“¦ DTO ì •ì˜\n",
    "# ==============================\n",
    "\n",
    "@dataclass\n",
    "class PopupImageDTO:\n",
    "    imageUrl: str\n",
    "    sortOrder: int\n",
    "\n",
    "@dataclass\n",
    "class PopupUploadDTO:\n",
    "    name: str\n",
    "    startDate: str\n",
    "    endDate: str\n",
    "    openTime: Optional[str]\n",
    "    closeTime: Optional[str]\n",
    "    address: str\n",
    "    roadAddress: Optional[str]\n",
    "    longitude: Optional[float]\n",
    "    latitude: Optional[float]\n",
    "    region: str\n",
    "    geocodingQuery: str\n",
    "    instaPostId: str\n",
    "    instaPostUrl: str\n",
    "    captionSummary: str\n",
    "    caption: str\n",
    "    mediaType: str\n",
    "    imageUrl: Optional[str]\n",
    "    imageList: List[PopupImageDTO]\n",
    "    recommendIds: List[int]\n",
    "    isActive: bool = True\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ§­ ìœ í‹¸ í•¨ìˆ˜\n",
    "# ==============================\n",
    "\n",
    "def convert_to_public_path(local_path: str) -> str:\n",
    "    \"\"\"ì ˆëŒ€ ê²½ë¡œë¥¼ /images/... ìƒëŒ€ ê²½ë¡œë¡œ ë³€í™˜\"\"\"\n",
    "    if \"/images/\" in local_path:\n",
    "        idx = local_path.index(\"/images/\")\n",
    "        return local_path[idx:]\n",
    "    return local_path\n",
    "\n",
    "\n",
    "def build_image_list(image_paths: List[str]) -> List[PopupImageDTO]:\n",
    "    \"\"\"image_paths â†’ DTO ë¦¬ìŠ¤íŠ¸ ë³€í™˜\"\"\"\n",
    "    return [\n",
    "        PopupImageDTO(\n",
    "            imageUrl=convert_to_public_path(path),\n",
    "            sortOrder=idx\n",
    "        )\n",
    "        for idx, path in enumerate(image_paths)\n",
    "    ]\n",
    "\n",
    "\n",
    "def build_payload(item: dict) -> PopupUploadDTO:\n",
    "    \"\"\"ì›ë³¸ dict â†’ DTO ë³€í™˜\"\"\"\n",
    "    image_paths = item.get(\"image_paths\", [])\n",
    "    image_list = build_image_list(image_paths)\n",
    "    image_urls = item.get(\"image_url\", [])\n",
    "    image_url_first = image_urls[0] if image_urls else None\n",
    "\n",
    "    return PopupUploadDTO(\n",
    "        name=item.get(\"name\"),\n",
    "        startDate=item.get(\"start_date\"),\n",
    "        endDate=item.get(\"end_date\"),\n",
    "        openTime=item.get(\"open_time\"),\n",
    "        closeTime=item.get(\"close_time\"),\n",
    "        address=item.get(\"address\"),\n",
    "        roadAddress=item.get(\"road_address\"),\n",
    "        longitude=item.get(\"longitude\"),\n",
    "        latitude=item.get(\"latitude\"),\n",
    "        region=item.get(\"region\"),\n",
    "        geocodingQuery=item.get(\"geocoding_query\"),\n",
    "        instaPostId=item.get(\"insta_post_id\"),\n",
    "        instaPostUrl=item.get(\"insta_post_url\"),\n",
    "        captionSummary=item.get(\"caption_summary\"),\n",
    "        caption=item.get(\"caption\"),\n",
    "        mediaType=item.get(\"media_type\"),\n",
    "        imageUrl=image_url_first,\n",
    "        imageList=image_list,\n",
    "        recommendIds=[1],\n",
    "    )\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ¬ Mysql ì—…ë¡œë“œ í´ë˜ìŠ¤\n",
    "# ==============================\n",
    "\n",
    "class Mysql:\n",
    "    @staticmethod\n",
    "    def send_popup(item: dict, api_url: str) -> bool:\n",
    "        \"\"\"Vision í†µê³¼í•œ íŒì—… ì •ë³´ë¥¼ APIë¡œ ì—…ë¡œë“œ\"\"\"\n",
    "        payload_dto = build_payload(item)\n",
    "        payload = json.loads(json.dumps(payload_dto, default=lambda o: o.__dict__))\n",
    "\n",
    "        print(\"\\nğŸ“¤ ì—…ë¡œë“œ ìš”ì²­ payload:\")\n",
    "        print(json.dumps(payload, ensure_ascii=False, indent=2))\n",
    "\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = requests.post(api_url, json=payload, headers=headers, timeout=10)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… ì—…ë¡œë“œ ì„±ê³µ: {payload_dto.instaPostId}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨ ({response.status_code}): {payload_dto.instaPostId}\")\n",
    "            print(response.text)\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def play():\n",
    "        # âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ\n",
    "        BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "        env_path = os.path.join(BASE_DIR, \".env\")\n",
    "        file_path = os.path.join(BASE_DIR, \"geo.json\")\n",
    "        load_dotenv(dotenv_path=env_path, override=True)\n",
    "\n",
    "        API_URL = os.getenv(\"POPUP_API_URL\", \"https://poppang.co.kr/api/v1/popup\")\n",
    "\n",
    "        print(f\"ğŸ“‚ BASE_DIR: {BASE_DIR}\")\n",
    "        print(f\"ğŸ“„ JSON ê²½ë¡œ: {file_path} â†’ ì¡´ì¬? {os.path.exists(file_path)}\")\n",
    "        print(f\"ğŸŒ API URL: {API_URL}\")\n",
    "\n",
    "        if not os.path.exists(file_path):\n",
    "            print(\"âŒ popup_with_geo.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        total = len(data)\n",
    "        inserted = 0\n",
    "        skipped = 0\n",
    "        human_skipped = 0\n",
    "\n",
    "        for item in data:\n",
    "            insta_post_id = item.get(\"insta_post_id\")\n",
    "            media_type = item.get(\"media_type\")\n",
    "            image_paths = item.get(\"image_paths\", [])\n",
    "\n",
    "            print(f\"\\nğŸ“Œ ì²˜ë¦¬ì¤‘: insta_post_id={insta_post_id}\")\n",
    "            print(f\"ğŸ“¸ ì´ë¯¸ì§€ ê²½ë¡œ: {image_paths}\")\n",
    "\n",
    "            # ğŸ¥ VIDEO Vision ìŠ¤í‚µ\n",
    "            if media_type == \"VIDEO\":\n",
    "                print(f\"ğŸ¥ VIDEO íƒ€ì… â†’ Vision ê²€ì‚¬ ìŠ¤í‚µ\")\n",
    "                if Mysql.send_popup(item, API_URL):\n",
    "                    inserted += 1\n",
    "                else:\n",
    "                    skipped += 1\n",
    "                continue\n",
    "\n",
    "            # ğŸ§  Vision ê²€ì‚¬ (IMAGE, CAROUSEL_ALBUM)\n",
    "            if image_paths:\n",
    "                try:\n",
    "                    has_human = VisionAPI.contains_human_in_all_files(image_paths)\n",
    "                    if has_human:\n",
    "                        human_skipped += 1\n",
    "                        print(f\"ğŸš« Vision ê°ì§€ë¨ â†’ ì—…ë¡œë“œ ìŠ¤í‚µ insta_post_id={insta_post_id}\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Vision ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜: {e}\")\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"âš ï¸ image_paths ë¹„ì–´ìˆìŒ â†’ Vision ê²€ì‚¬ ìŠ¤í‚µ\")\n",
    "\n",
    "            # âœ… Vision í†µê³¼ í›„ ì—…ë¡œë“œ\n",
    "            if Mysql.send_popup(item, API_URL):\n",
    "                inserted += 1\n",
    "            else:\n",
    "                skipped += 1\n",
    "\n",
    "        print(\"\\n===============================\")\n",
    "        print(f\"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {inserted}/{total}\")\n",
    "        print(f\"ğŸš« Vision í•„í„°ë¡œ ìŠ¤í‚µ: {human_skipped}\")\n",
    "        print(f\"âš ï¸ ì˜¤ë¥˜ ë˜ëŠ” ê¸°íƒ€ ìŠ¤í‚µ: {skipped}\")\n",
    "        print(\"===============================\")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# ğŸ ì‹¤í–‰\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    Mysql.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcebbce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rest-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
