
# streamlit run 7.\ streamlitChat.py 
import streamlit as st
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Pinecone as PineconeVectorStore
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# 1ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ ë¡œë“œ (.envì—ì„œ API í‚¤ ë¶ˆëŸ¬ì˜´)
load_dotenv()

# í•™ìŠµì‹œí‚¨ AI
def LLM(user_message):

    # 2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸
    embedding = OpenAIEmbeddings(model="text-embedding-3-large")

    # 3ï¸âƒ£ Pinecone ê¸°ì¡´ ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
    index_name = "tax-index"
    database = PineconeVectorStore.from_existing_index(
        index_name=index_name,
        embedding=embedding
    )

    # 4ï¸âƒ£ LLM ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o")

    # 5ï¸âƒ£ ì‚¬ìš©ì ë‹¨ì–´ â†’ ì„¸ë¬´ ìš©ì–´ ë³€í™˜ìš© ì‚¬ì „
    dictionary = {
        "ì§ì¥ì¸": "ê±°ì£¼ì",
        "ê·¼ë¡œì": "ê±°ì£¼ì",
        "ì›”ê¸‰": "ê·¼ë¡œì†Œë“",
        "ì„¸ê¸ˆ": "ì†Œë“ì„¸"
    }

    # 6ï¸âƒ£ ì§ˆë¬¸ì„ ë³€í™˜í•˜ê¸° ìœ„í•œ ì‚¬ì „ ì²´ì¸
    prompt = ChatPromptTemplate.from_template("""
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
    ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
    ê·¸ëŸ° ê²½ìš°ì—ëŠ” ì§ˆë¬¸ë§Œ ë¦¬í„´í•´ì£¼ì„¸ìš”.

    ì‚¬ì „: {dictionary}
    ì§ˆë¬¸: {question}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()

    # 7ï¸âƒ£ ë³€í™˜ëœ ì§ˆë¬¸ ìƒì„±
    new_query = dictionary_chain.invoke({
        "dictionary": dictionary,
        "question": user_message
    })

    # 8ï¸âƒ£ ë²¡í„°DBì—ì„œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    retrieved_docs = database.similarity_search(new_query, k=3)

    # 9ï¸âƒ£ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])

    # ğŸ”Ÿ ìµœì¢… RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    rag_prompt_template = PromptTemplate(
        template="""
        You are a helpful assistant that can answer questions about Korean tax law.
        Use the following context to answer accurately and concisely in Korean.

        Context:
        {context}

        Question:
        {question}

        Answer:
        """,
        input_variables=["context", "question"]
    )

    # 11ï¸âƒ£ ì²´ì¸ ì—°ê²°
    rag_chain = rag_prompt_template | llm | StrOutputParser()

    # 12ï¸âƒ£ ë‹µë³€ ìƒì„±
    ai_message = rag_chain.invoke({
        "context": context_text,
        "question": new_query
    })

    return ai_message

# ì—¬ê¸°ì„œë¶€í„°ëŠ” ê·¸ëƒ¥ UI
st.set_page_config(page_title="ì†Œë“ì„¸ ì±—ë´‡", page_icon="ğŸ“š")
st.title("ğŸ“š ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤!")

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

# print(f"before == {st.session_state.message_list}")
for message in st.session_state.message_list:
    with st.chat_message(message['role']):
        st.write(message['content'])

if user_question := st.chat_input(placeholder="ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        # AI ë‹µë³€
        ai_message = LLM(user_question)
        with st.chat_message("ai"):
            st.write(ai_message)
        st.session_state.message_list.append({"role": "ai", "content": ai_message})


        
# print(f"after == {st.session_state.message_list}")